{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "de2c0274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "80e226c7-dfb7-4a2b-a1b3-c3ebfd05b4fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_date_to_unix(x):\n",
    "    \"\"\"\n",
    "    Convert datetime to unix time in milliseconds.\n",
    "    \"\"\"\n",
    "    dt_obj = datetime.datetime.strptime(str(x), '%Y-%m-%d %H:%M:%S')\n",
    "    dt_obj = int(dt_obj.timestamp() * 1000)\n",
    "    return dt_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "cf4be7d2-9db6-471e-acfb-7c6d9d8583e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_city_coordinates(city_name: str):\n",
    "    \"\"\"\n",
    "    Takes city name and returns its latitude and longitude (rounded to 2 digits after dot).\n",
    "    \"\"\" \n",
    "    # Initialize Nominatim API (for getting lat and long of the city)\n",
    "    geolocator = Nominatim(user_agent=\"MyApp\")\n",
    "    city = geolocator.geocode(city_name)\n",
    "\n",
    "    latitude = round(city.latitude, 2)\n",
    "    longitude = round(city.longitude, 2)\n",
    "    \n",
    "    return latitude, longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf06ca9-ae24-4fc3-83f4-a6ea7610e0a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Representing the Target cities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "70ed6a65-8fcf-401a-88ff-8d8803bd5554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('target_cities.json') as json_file:\n",
    "    target_cities = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5121c4a9-48ff-4d1b-938c-c7b18c50d0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU\n",
      "17\n",
      "US\n",
      "13\n",
      "Seattle\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for i in target_cities:\n",
    "    print(i)\n",
    "    print(len(target_cities[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5486cdda-68dc-496b-9182-7f6323c56f86",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EU': ['Amsterdam',\n",
      "        'Athina',\n",
      "        'Berlin',\n",
      "        'Gdansk',\n",
      "        'Kraków',\n",
      "        'London',\n",
      "        'Madrid',\n",
      "        'Marseille',\n",
      "        'Milano',\n",
      "        'München',\n",
      "        'Napoli',\n",
      "        'Paris',\n",
      "        'Sevilla',\n",
      "        'Stockholm',\n",
      "        'Tallinn',\n",
      "        'Varna',\n",
      "        'Wien'],\n",
      " 'Seattle': {'Seattle - Bellevue-SE 12th St': (47.6008630009392, -122.148397),\n",
      "             'Seattle - DARRINGTON - FIR ST (Darrington High School)': (48.2469,\n",
      "                                                                        -121.6031),\n",
      "             'Seattle - KENT - JAMES & CENTRAL': (47.386111, -122.230278),\n",
      "             'Seattle - LAKE FOREST PARK TOWNE CENTER': (47.755, -122.2806),\n",
      "             'Seattle - MARYSVILLE - 7TH AVE (Marysville Junior High)': (48.054315,\n",
      "                                                                         -122.171529),\n",
      "             'Seattle - NORTH BEND - NORTH BEND WAY': (47.49022, -121.77278),\n",
      "             'Seattle - SEATTLE - BEACON HILL': (47.568236, -122.308628),\n",
      "             'Seattle - SEATTLE - DUWAMISH': (47.55975, -122.338265),\n",
      "             'Seattle - SEATTLE - SOUTH PARK #2': (47.53091, -122.3208),\n",
      "             'Seattle - Seattle-10th & Weller': (47.597222, -122.319722),\n",
      "             'Seattle - TACOMA - ALEXANDER AVE': (47.2656, -122.3858),\n",
      "             'Seattle - TACOMA - L STREET': (47.1864, -122.4517),\n",
      "             'Seattle - Tacoma-S 36th St': (47.22634, -122.46256),\n",
      "             'Seattle - Tukwila Allentown': (47.4985350009395, -122.278385),\n",
      "             'Seattle - Tulalip-Totem Beach Rd': (48.065339, -122.285194)},\n",
      " 'US': ['Albuquerque',\n",
      "        'Atlanta',\n",
      "        'Chicago',\n",
      "        'Columbus',\n",
      "        'Dallas',\n",
      "        'Denver',\n",
      "        'Houston',\n",
      "        'Los Angeles',\n",
      "        'New York',\n",
      "        'Phoenix-Mesa',\n",
      "        'Salt Lake City',\n",
      "        'San Francisco',\n",
      "        'Tampa']}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(target_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "74784882-004b-47be-a4c0-cdb742da69b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(\"target_cities.json\", \"w\") as json_file:\n",
    "#     json.dump(target_cities, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78abf0b0-7a06-417b-acd6-ebe0719e4f21",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ALL target cities on the one map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1fcd0fb1-ca9a-435c-8f4b-b89e9d44096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folium map centered on the first location in the list\n",
    "map = folium.Map(location=[42.57, -44.092], zoom_start=3)\n",
    "\n",
    "for city in target_cities[\"EU\"]:\n",
    "    latitude, longitude = get_city_coordinates(city)\n",
    "    folium.Marker(location=[latitude, longitude]).add_to(map)\n",
    "    \n",
    "for city in target_cities[\"US\"]:\n",
    "    latitude, longitude = get_city_coordinates(city)\n",
    "    folium.Marker(location=[latitude, longitude]).add_to(map)\n",
    "\n",
    "for city in target_cities[\"Seattle\"]:\n",
    "    latitude, longitude = target_cities[\"Seattle\"][city]\n",
    "    folium.Marker(location=[latitude, longitude]).add_to(map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "map.save(\"map_all_target_cities.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e211e-4ae8-4e0b-b144-df283c697f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a9f7134-c38b-429e-92f1-f7ddb95a3653",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "a8290ee7-d891-4f6d-94ab-4140bbde0706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in target_cities[\"Seattle\"]:\n",
    "#     a, b = target_cities[\"Seattle\"][i]\n",
    "#     target_cities[\"Seattle\"][i] = round(a, 5), round(b, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "ea09c2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('target_cities.json', \"w\") as json_file:\n",
    "    json.dump(target_cities, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf176d22-f807-44f2-9a23-895924e73a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast\n",
    "\n",
    "# string = \"(47.1864, -122.4517)\"\n",
    "# t = ast.literal_eval(string)\n",
    "# t, type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706efd6-cff8-43fb-bcc9-3d7a419c3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seattle_info = seattle_df[[\"city_name\", \"latitude\", \"longitude\"]].drop_duplicates()\n",
    "# for city_name, lat, long in seattle_info.values:\n",
    "#     target_cities[\"Seattle\"][city_name] = (lat, long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020905c-06bb-440c-bf99-d0caa6affc28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# [EEA](https://discomap.eea.europa.eu/map/fme/AirQualityExport.htm)\n",
    "## EEA means European Environmental Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c42cc4-7de7-4247-a071-d1b47f912309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_daily(df, pollutant: str):\n",
    "    \"\"\"\n",
    "    Returns DataFrame where pollutant column is resampled to days and rounded.\n",
    "    \"\"\"\n",
    "    res_df = df.copy()\n",
    "    # convert dates in 'time' column\n",
    "    res_df[\"date\"] = pd.to_datetime(res_df[\"date\"])\n",
    "    \n",
    "    # I want data daily, not hourly (mean per each day = 1 datarow per 1 day)\n",
    "    res_df = res_df.set_index('date')\n",
    "    res_df = res_df[pollutant].resample('1d').mean().reset_index()\n",
    "    res_df[pollutant] = res_df[pollutant].fillna(res_df[pollutant].median())\n",
    "    res_df[pollutant] = res_df[pollutant].apply(lambda x: round(x, 0))\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "391db0ab-3ddd-46b4-8af8-8434190f3747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_fullest_csv(csv_links: list, year: str):\n",
    "    candidates = [link for link in csv_links if str(year) in link]\n",
    "    biggest_df = pd.read_csv(candidates[0])\n",
    "    for link in candidates[1:]:\n",
    "        _df = pd.read_csv(link)\n",
    "        if len(biggest_df) < len(_df):\n",
    "            biggest_df = _df\n",
    "    return biggest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "936af3d9-1fe5-4ae2-ad51-020922d69f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_air_quality_from_eea(city_name: str,\n",
    "                             pollutant: str,\n",
    "                             start_year: str,\n",
    "                             end_year: str):\n",
    "    \"\"\"\n",
    "    Takes city name, daterange and returns pandas DataFrame with daily air quality data.\n",
    "    It parses data by 1-year batches, so please specify years, not dates. (example: \"2014\", \"2022\"...)\n",
    "    \n",
    "    EEA means European Environmental Agency. So it has data for Europe Union countries ONLY.\n",
    "    \"\"\"\n",
    "    start_of_cell = time.time()\n",
    "    \n",
    "    params = {\n",
    "        'CountryCode': '',\n",
    "        'CityName': city_name,\n",
    "        'Pollutant': pollutant.upper(),\n",
    "        'Year_from': start_year,\n",
    "        'Year_to': end_year,\n",
    "        'Station': '',\n",
    "        'Source': 'All',\n",
    "        'Samplingpoint': '',\n",
    "        'Output': 'TEXT',\n",
    "        'UpdateDate': '',\n",
    "        'TimeCoverage': 'Year'\n",
    "    }\n",
    "\n",
    "    # observations endpoint\n",
    "    base_url = \"https://fme.discomap.eea.europa.eu/fmedatastreaming/AirQualityDownload/AQData_Extract.fmw?\"\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    response.encoding = response.apparent_encoding\n",
    "    csv_links = response.text.split(\"\\r\\n\")\n",
    "    \n",
    "    res_df = pd.DataFrame()\n",
    "    target_year = int(start_year)\n",
    "    \n",
    "    for year in range(int(start_year), int(end_year) + 1):\n",
    "        try:\n",
    "            # find the fullest, the biggest csv file with observations for this particular year\n",
    "            _df = find_fullest_csv(csv_links, year)\n",
    "            # append it to res_df\n",
    "            res_df = pd.concat([res_df, _df])\n",
    "        except IndexError:\n",
    "            print(f\"!! Missing data for {year} for {city} city.\")\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    pollutant = pollutant.lower()\n",
    "    if pollutant == \"pm2.5\":\n",
    "        pollutant = \"pm2_5\"\n",
    "        \n",
    "    res_df = res_df.rename(columns={\n",
    "        'DatetimeBegin': 'date',\n",
    "        'Concentration': pollutant        \n",
    "    })\n",
    "    \n",
    "    # cut timezones info\n",
    "    res_df['date'] = res_df['date'].apply(lambda x: x[:-6])\n",
    "    # convert dates in 'time' column\n",
    "    res_df['date'] = pd.to_datetime(res_df['date'])\n",
    "    \n",
    "    res_df = convert_to_daily(res_df, pollutant)\n",
    "    \n",
    "    res_df['city_name'] = city_name\n",
    "    res_df = res_df[['city_name', 'date', pollutant.lower()]]\n",
    "    \n",
    "    end_of_cell = time.time()\n",
    "    \n",
    "    print(f\"Processed {pollutant.upper()} for {city_name} since {start_year} till {end_year}.\")\n",
    "    print(f\"Took {round(end_of_cell - start_of_cell, 2)} sec.\\n\")\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33a220c-c241-4e55-963a-225d4c6ea70e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PM2_5 for Barcelona since 2013 till 2013.\n",
      "Took 11.16 sec.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>date</th>\n",
       "      <th>pm2_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barcelona</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barcelona</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barcelona</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   city_name       date  pm2_5\n",
       "0  Barcelona 2013-01-01   19.0\n",
       "1  Barcelona 2013-01-02   23.0\n",
       "2  Barcelona 2013-01-03   28.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eea_ = get_air_quality_from_eea(\n",
    "    city_name=\"Barcelona\", pollutant=\"PM2.5\",\n",
    "    start_year=\"2013\", end_year=\"2013\"\n",
    ")\n",
    "\n",
    "df_eea_.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3023c2db-034c-4a70-8ba7-401a61d30acb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amsterdam',\n",
       " 'Athina',\n",
       " 'Berlin',\n",
       " 'Gdansk',\n",
       " 'Kraków',\n",
       " 'London',\n",
       " 'Madrid',\n",
       " 'Marseille',\n",
       " 'Milano',\n",
       " 'München',\n",
       " 'Napoli',\n",
       " 'Paris',\n",
       " 'Sevilla',\n",
       " 'Stockholm',\n",
       " 'Tallinn',\n",
       " 'Varna',\n",
       " 'Wien']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cities[\"EU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f280faa-38d8-4d39-9080-172b6169aff5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PM2_5 for Amsterdam since 2013 till 2023.\n",
      "Took 61.22 sec.\n",
      "\n",
      "Processed PM2_5 for Athina since 2013 till 2023.\n",
      "Took 25.29 sec.\n",
      "\n",
      "Processed PM2_5 for Berlin since 2013 till 2023.\n",
      "Took 42.02 sec.\n",
      "\n",
      "Processed PM2_5 for Gdansk since 2013 till 2023.\n",
      "Took 12.21 sec.\n",
      "\n",
      "Processed PM2_5 for Kraków since 2013 till 2023.\n",
      "Took 17.68 sec.\n",
      "\n",
      "Processed PM2_5 for London since 2013 till 2023.\n",
      "Took 70.08 sec.\n",
      "\n",
      "Processed PM2_5 for Madrid since 2013 till 2023.\n",
      "Took 56.7 sec.\n",
      "\n",
      "Processed PM2_5 for Marseille since 2013 till 2023.\n",
      "Took 21.62 sec.\n",
      "\n",
      "Processed PM2_5 for Milano since 2013 till 2023.\n",
      "Took 22.92 sec.\n",
      "\n",
      "Processed PM2_5 for München since 2013 till 2023.\n",
      "Took 48.32 sec.\n",
      "\n",
      "Processed PM2_5 for Napoli since 2013 till 2023.\n",
      "Took 48.52 sec.\n",
      "\n",
      "Processed PM2_5 for Paris since 2013 till 2023.\n",
      "Took 54.6 sec.\n",
      "\n",
      "Processed PM2_5 for Sevilla since 2013 till 2023.\n",
      "Took 9.27 sec.\n",
      "\n",
      "Processed PM2_5 for Stockholm since 2013 till 2023.\n",
      "Took 62.78 sec.\n",
      "\n",
      "Processed PM2_5 for Tallinn since 2013 till 2023.\n",
      "Took 5.84 sec.\n",
      "\n",
      "!! Missing data for 2023 for Varna city.\n",
      "Processed PM2_5 for Varna since 2013 till 2023.\n",
      "Took 2.95 sec.\n",
      "\n",
      "Processed PM2_5 for Wien since 2013 till 2023.\n",
      "Took 77.5 sec.\n",
      "\n",
      "Processed PM2.5 for EU cities since 2013 till 2023.\n",
      "Took 639.55 sec.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_of_cell = time.time()\n",
    "\n",
    "pollutant = \"PM2.5\" # it will become \"pm2_5\" anyway\n",
    "start_year = 2013\n",
    "end_year = 2023\n",
    "\n",
    "df_eu = pd.DataFrame()\n",
    "\n",
    "for city in target_cities[\"EU\"]:\n",
    "    df_ = get_air_quality_from_eea(\n",
    "        city_name=city, pollutant=pollutant,\n",
    "        start_year=start_year, end_year=end_year\n",
    "    )\n",
    "    df_eu = pd.concat([df_eu, df_])\n",
    "\n",
    "end_of_cell = time.time()\n",
    "print(\"-\" * 64)\n",
    "print(f\"Processed {pollutant.upper()} for EU cities since {start_year} till {end_year}.\")\n",
    "print(f\"Took {round(end_of_cell - start_of_cell, 2)} sec.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4821ecad-0052-428f-b6cc-d1e5fb2b2b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>date</th>\n",
       "      <th>pm2_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>Wien</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>Wien</td>\n",
       "      <td>2023-04-08</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>Wien</td>\n",
       "      <td>2023-04-09</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>Wien</td>\n",
       "      <td>2023-04-10</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>Wien</td>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63548 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      city_name       date  pm2_5\n",
       "0     Amsterdam 2013-01-01   14.0\n",
       "1     Amsterdam 2013-01-02    8.0\n",
       "2     Amsterdam 2013-01-03   12.0\n",
       "3     Amsterdam 2013-01-04   12.0\n",
       "4     Amsterdam 2013-01-05   14.0\n",
       "...         ...        ...    ...\n",
       "3748       Wien 2023-04-07   20.0\n",
       "3749       Wien 2023-04-08   10.0\n",
       "3750       Wien 2023-04-09   15.0\n",
       "3751       Wien 2023-04-10   18.0\n",
       "3752       Wien 2023-04-11   17.0\n",
       "\n",
       "[63548 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81d3b003-175d-47eb-9b75-7e3d5403783a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eu.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5589c767-a0b3-4530-98d5-7446b6904f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_eu.to_csv(\"data/backfill_pm2_5_eu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a0961-56d5-42bd-bbf8-b639acfcfe42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# [USEPA](https://aqs.epa.gov/aqsweb/documents/data_api.html#daily)\n",
    "## USEPA means United States Environmental Protection Agency\n",
    "[Manual downloading](https://www.epa.gov/outdoor-air-quality-data/download-daily-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "244b6d79-01d1-487a-811b-2b7a83bd0a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "city_code_dict = {}\n",
    "pollutant_dict = {\n",
    "    'CO': '42101',\n",
    "    'SO2': '42401',\n",
    "    'NO2': '42602',\n",
    "    'O3': '44201',\n",
    "    'PM10': '81102',\n",
    "    'PM2.5': '88101'\n",
    "}\n",
    "\n",
    "def get_city_code(city_name: str):\n",
    "    \"Encodes city name to be used later for data parsing using USEPA.\"\n",
    "    if city_code_dict:\n",
    "        city_full = [i for i in city_code_dict.keys() if city_name in i][0]\n",
    "        return city_code_dict[city_full]\n",
    "    else:\n",
    "        params = {\n",
    "            \"email\": \"test@aqs.api\",\n",
    "            \"key\": \"test\"\n",
    "        }\n",
    "        response = requests.get(\"https://aqs.epa.gov/data/api/list/cbsas?\", params)\n",
    "        response_json = response.json()\n",
    "        data = response_json[\"Data\"]\n",
    "        for item in data:\n",
    "            city_code_dict[item['value_represented']] = item['code']\n",
    "        \n",
    "        return get_city_code(city_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a524113-8c5b-4142-be0a-49167f1fa0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_date_intervals(start_date, end_date):\n",
    "    start_dt = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    date_intervals = []\n",
    "    for year in range(start_dt.year, end_dt.year + 1):\n",
    "        year_start = datetime.datetime(year, 1, 1)\n",
    "        year_end = datetime.datetime(year, 12, 31)\n",
    "        interval_start = max(start_dt, year_start)\n",
    "        interval_end = min(end_dt, year_end)\n",
    "        if interval_start < interval_end:\n",
    "            date_intervals.append((interval_start.strftime('%Y%m%d'), interval_end.strftime('%Y%m%d')))\n",
    "    return date_intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "821fc3b5-40d7-464b-a509-9df6ec055bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_air_quality_from_usepa(city_name: str,\n",
    "                               pollutant: str,\n",
    "                               start_date: str,\n",
    "                               end_date: str):\n",
    "    \"\"\"\n",
    "    Takes city name, daterange and returns pandas DataFrame with daily air quality data.\n",
    "    \n",
    "    USEPA means United States Environmental Protection Agency. So it has data for US ONLY.\n",
    "    \"\"\"\n",
    "    start_of_cell = time.time()\n",
    "        \n",
    "    res_df = pd.DataFrame()\n",
    "    \n",
    "#     # to print 'Success' log only once.\n",
    "#     was = False\n",
    "    for start_date_, end_date_ in make_date_intervals(start_date, end_date):\n",
    "        params = {\n",
    "            \"email\": \"test@aqs.api\",\n",
    "            \"key\": \"test\",\n",
    "            \"param\": pollutant_dict[pollutant.upper().replace(\"_\", \".\")], # encoded pollutant \n",
    "            \"bdate\": start_date_,\n",
    "            \"edate\": end_date_,\n",
    "            \"cbsa\": get_city_code(city_name) # Core-based statistical area\n",
    "        }\n",
    "\n",
    "        # observations endpoint\n",
    "        base_url = \"https://aqs.epa.gov/data/api/dailyData/byCBSA?\" \n",
    "\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response_json = response.json()\n",
    "        # if not was:\n",
    "        #     print(response_json[\"Header\"][0][\"status\"])\n",
    "        #     was = True\n",
    "        \n",
    "        df_ = pd.DataFrame(response_json[\"Data\"])\n",
    "        \n",
    "        pollutant = pollutant.lower()\n",
    "        \n",
    "        if pollutant == \"pm2.5\":\n",
    "            pollutant = \"pm2_5\"\n",
    "        df_ = df_.rename(columns={\n",
    "            'date_local': 'date',\n",
    "            'arithmetic_mean': pollutant        \n",
    "        })\n",
    "\n",
    "        # convert dates in 'date' column\n",
    "        df_['date'] = pd.to_datetime(df_['date'])\n",
    "        df_['city_name'] = city_name    \n",
    "       \n",
    "        df_ = df_[['city_name', 'date', pollutant]]\n",
    "\n",
    "        res_df = pd.concat([res_df, df_])\n",
    "    \n",
    "    # there are duplicated rows (several records for the same day and station). get rid of it.\n",
    "    res_df = res_df.groupby(['date', 'city_name'], as_index=False)[pollutant].mean()\n",
    "    res_df[pollutant] = round(res_df[pollutant], 1)  \n",
    "    \n",
    "    end_of_cell = time.time()\n",
    "    print(f\"Processed {pollutant.upper()} for {city_name} since {start_date} till {end_date}.\")\n",
    "    print(f\"Took {round(end_of_cell - start_of_cell, 2)} sec.\\n\")\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "76393f72-7120-499e-9b44-b3bbe339f337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PM2_5 for Albuquerque since 2013-07-01 till 2014-01-01.\n",
      "Took 2.16 sec.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_usepa_ = get_air_quality_from_usepa(city_name=\"Albuquerque\", pollutant=\"PM2.5\",\n",
    "                                       start_date=\"2013-07-01\", end_date=\"2014-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3e576193-7b53-4928-8d13-1dcf04921e50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city_name</th>\n",
       "      <th>pm2_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-07-02</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-07-03</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-07-04</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>12.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2013-12-27</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2013-12-28</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>14.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2013-12-29</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2013-12-30</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date    city_name  pm2_5\n",
       "0   2013-07-01  Albuquerque    4.4\n",
       "1   2013-07-02  Albuquerque   12.4\n",
       "2   2013-07-03  Albuquerque    5.6\n",
       "3   2013-07-04  Albuquerque   12.3\n",
       "4   2013-07-05  Albuquerque   11.2\n",
       "..         ...          ...    ...\n",
       "179 2013-12-27  Albuquerque    8.6\n",
       "180 2013-12-28  Albuquerque   14.7\n",
       "181 2013-12-29  Albuquerque    9.3\n",
       "182 2013-12-30  Albuquerque    9.3\n",
       "183 2013-12-31  Albuquerque   13.4\n",
       "\n",
       "[184 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_usepa_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "210059d4-d2a0-4cf1-a7a5-ff70a93fbcfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albuquerque',\n",
       " 'Atlanta',\n",
       " 'Chicago',\n",
       " 'Columbus',\n",
       " 'Dallas',\n",
       " 'Denver',\n",
       " 'Houston',\n",
       " 'Los Angeles',\n",
       " 'New York',\n",
       " 'Phoenix-Mesa',\n",
       " 'Salt Lake City',\n",
       " 'San Francisco',\n",
       " 'Tampa']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cities[\"US\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1bfc14c-233f-42e7-a844-c4f9adc2dcbb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PM2_5 for Albuquerque since 2013-01-01 till 2023-01-01.\n",
      "Took 63.28 sec.\n",
      "\n",
      "Processed PM2_5 for Atlanta since 2013-01-01 till 2023-01-01.\n",
      "Took 53.23 sec.\n",
      "\n",
      "Processed PM2_5 for Chicago since 2013-01-01 till 2023-01-01.\n",
      "Took 120.86 sec.\n",
      "\n",
      "Processed PM2_5 for Columbus since 2013-01-01 till 2023-01-01.\n",
      "Took 42.43 sec.\n",
      "\n",
      "Processed PM2_5 for Dallas since 2013-01-01 till 2023-01-01.\n",
      "Took 44.47 sec.\n",
      "\n",
      "Processed PM2_5 for Denver since 2013-01-01 till 2023-01-01.\n",
      "Took 72.45 sec.\n",
      "\n",
      "Processed PM2_5 for Houston since 2013-01-01 till 2023-01-01.\n",
      "Took 54.07 sec.\n",
      "\n",
      "Processed PM2_5 for Los Angeles since 2013-01-01 till 2023-01-01.\n",
      "Took 87.88 sec.\n",
      "\n",
      "Processed PM2_5 for New York since 2013-01-01 till 2023-01-01.\n",
      "Took 138.85 sec.\n",
      "\n",
      "Processed PM2_5 for Phoenix-Mesa since 2013-01-01 till 2023-01-01.\n",
      "Took 121.5 sec.\n",
      "\n",
      "Processed PM2_5 for Salt Lake City since 2013-01-01 till 2023-01-01.\n",
      "Took 109.5 sec.\n",
      "\n",
      "Processed PM2_5 for San Francisco since 2013-01-01 till 2023-01-01.\n",
      "Took 100.42 sec.\n",
      "\n",
      "Processed PM2_5 for Tampa since 2013-01-01 till 2023-01-01.\n",
      "Took 39.12 sec.\n",
      "\n",
      "Processed PM2.5 for US cities since 2013 till 2023.\n",
      "Took 1048.54 sec.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_of_cell = time.time()\n",
    "\n",
    "pollutant = \"PM2.5\"\n",
    "\n",
    "start_date = \"2013-01-01\"\n",
    "end_date = \"2023-01-01\"\n",
    "\n",
    "df_us = pd.DataFrame()\n",
    "\n",
    "for city in target_cities[\"US\"]:\n",
    "    df_ = get_air_quality_from_usepa(\n",
    "        city_name=city, pollutant=pollutant,\n",
    "        start_date=start_date, end_date=end_date\n",
    "        )\n",
    "    df_us = pd.concat([df_us, df_])\n",
    "    \n",
    "end_of_cell = time.time()\n",
    "print(\"-\" * 64)\n",
    "print(f\"Processed {pollutant.upper()} for US cities since {start_year} till {end_year}.\")\n",
    "print(f\"Took {round(end_of_cell - start_of_cell, 2)} sec.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d4750e9-6c9f-4890-a5b1-857efcc52f11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city_name</th>\n",
       "      <th>pm2_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46037 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date    city_name  pm2_5\n",
       "0    2013-01-01  Albuquerque    6.8\n",
       "1    2013-01-02  Albuquerque    8.4\n",
       "2    2013-01-03  Albuquerque    7.8\n",
       "3    2013-01-04  Albuquerque    9.2\n",
       "4    2013-01-05  Albuquerque   12.2\n",
       "...         ...          ...    ...\n",
       "3609 2022-12-27        Tampa    6.8\n",
       "3610 2022-12-28        Tampa    6.9\n",
       "3611 2022-12-29        Tampa    5.2\n",
       "3612 2022-12-30        Tampa    5.4\n",
       "3613 2022-12-31        Tampa    6.2\n",
       "\n",
       "[46037 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fc0244c1-ac75-48f0-bc81-a008c51b77cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d532d207-136b-448a-bd00-a2f4c715b7e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_us.to_csv(\"data/backfill_pm2_5_us.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74608c8d-c4bb-4b3b-9777-8df700fc710c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Processing special city - `Seattle`.\n",
    "### We need different stations across the Seattle. \n",
    "\n",
    "I downloaded daily `PM2.5` data manually from [here](https://www.epa.gov/outdoor-air-quality-data/download-daily-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "311c44d5-0c2e-4a36-8ae9-015e9cef6d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67901, 20)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seattle = pd.DataFrame()\n",
    "\n",
    "for year in range(2013, 2023 + 1):\n",
    "    df_ = pd.read_csv(f\"data/seattle_pm25_{year}.csv\")\n",
    "    df_seattle = pd.concat([df_seattle, df_])\n",
    "\n",
    "df_seattle = df_seattle.reset_index(drop=True)\n",
    "\n",
    "df_seattle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "0dd9ff2a-f978-4025-96ec-8dac354eeeee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Site ID</th>\n",
       "      <th>POC</th>\n",
       "      <th>Daily Mean PM2.5 Concentration</th>\n",
       "      <th>UNITS</th>\n",
       "      <th>DAILY_AQI_VALUE</th>\n",
       "      <th>Site Name</th>\n",
       "      <th>DAILY_OBS_COUNT</th>\n",
       "      <th>PERCENT_COMPLETE</th>\n",
       "      <th>AQS_PARAMETER_CODE</th>\n",
       "      <th>AQS_PARAMETER_DESC</th>\n",
       "      <th>CBSA_CODE</th>\n",
       "      <th>CBSA_NAME</th>\n",
       "      <th>STATE_CODE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY_CODE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>SITE_LATITUDE</th>\n",
       "      <th>SITE_LONGITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67899</th>\n",
       "      <td>04/02/2023</td>\n",
       "      <td>AirNow</td>\n",
       "      <td>530611007</td>\n",
       "      <td>5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>ug/m3 LC</td>\n",
       "      <td>20</td>\n",
       "      <td>MARYSVILLE - 7TH AVE (Marysville Junior High)</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88101</td>\n",
       "      <td>PM2.5 - Local Conditions</td>\n",
       "      <td>42660</td>\n",
       "      <td>Seattle-Tacoma-Bellevue, WA</td>\n",
       "      <td>53</td>\n",
       "      <td>Washington</td>\n",
       "      <td>61</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>48.054315</td>\n",
       "      <td>-122.171529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67900</th>\n",
       "      <td>04/03/2023</td>\n",
       "      <td>AirNow</td>\n",
       "      <td>530611007</td>\n",
       "      <td>5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>ug/m3 LC</td>\n",
       "      <td>20</td>\n",
       "      <td>MARYSVILLE - 7TH AVE (Marysville Junior High)</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88101</td>\n",
       "      <td>PM2.5 - Local Conditions</td>\n",
       "      <td>42660</td>\n",
       "      <td>Seattle-Tacoma-Bellevue, WA</td>\n",
       "      <td>53</td>\n",
       "      <td>Washington</td>\n",
       "      <td>61</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>48.054315</td>\n",
       "      <td>-122.171529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  Source    Site ID  POC  Daily Mean PM2.5 Concentration  \\\n",
       "67899  04/02/2023  AirNow  530611007    5                             4.8   \n",
       "67900  04/03/2023  AirNow  530611007    5                             4.8   \n",
       "\n",
       "          UNITS  DAILY_AQI_VALUE  \\\n",
       "67899  ug/m3 LC               20   \n",
       "67900  ug/m3 LC               20   \n",
       "\n",
       "                                           Site Name  DAILY_OBS_COUNT  \\\n",
       "67899  MARYSVILLE - 7TH AVE (Marysville Junior High)                1   \n",
       "67900  MARYSVILLE - 7TH AVE (Marysville Junior High)                1   \n",
       "\n",
       "       PERCENT_COMPLETE  AQS_PARAMETER_CODE        AQS_PARAMETER_DESC  \\\n",
       "67899             100.0               88101  PM2.5 - Local Conditions   \n",
       "67900             100.0               88101  PM2.5 - Local Conditions   \n",
       "\n",
       "       CBSA_CODE                    CBSA_NAME  STATE_CODE       STATE  \\\n",
       "67899      42660  Seattle-Tacoma-Bellevue, WA          53  Washington   \n",
       "67900      42660  Seattle-Tacoma-Bellevue, WA          53  Washington   \n",
       "\n",
       "       COUNTY_CODE     COUNTY  SITE_LATITUDE  SITE_LONGITUDE  \n",
       "67899           61  Snohomish      48.054315     -122.171529  \n",
       "67900           61  Snohomish      48.054315     -122.171529  "
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seattle.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "a7273c76-8a4d-4bfc-8929-75bbca9eeee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_seattle = df_seattle.rename(columns={\n",
    "    'Daily Mean PM2.5 Concentration': 'pm2_5',\n",
    "    'Date': 'date',\n",
    "    'SITE_LATITUDE': 'latitude',\n",
    "    'SITE_LONGITUDE': 'longitude',\n",
    "    'Site Name': 'city_name'\n",
    "})[['city_name', 'date', 'pm2_5', 'latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "56d8234e-225c-4988-a3eb-bd13f730f9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_seattle = df_seattle.drop_duplicates(subset=['date', 'city_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "84c21072-eae5-4866-81af-d03cad2f9157",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NORTH BEND - NORTH BEND WAY                                       3705\n",
       "TACOMA - L STREET                                                 3696\n",
       "SEATTLE - BEACON HILL                                             3691\n",
       "MARYSVILLE - 7TH AVE (Marysville Junior High)                     3648\n",
       "DARRINGTON - FIR ST (Darrington High School)                      3614\n",
       "SEATTLE - SOUTH PARK #2                                           3577\n",
       "TACOMA - ALEXANDER AVE                                            3569\n",
       "KENT - JAMES & CENTRAL                                            3556\n",
       "SEATTLE - DUWAMISH                                                3439\n",
       "Seattle-10th & Weller                                             3097\n",
       "LAKE FOREST PARK TOWNE CENTER                                     2999\n",
       "PUYALLUP - 128TH ST                                               2700\n",
       "Tacoma-S 36th St                                                  2574\n",
       "Bellevue-SE 12th St                                               2172\n",
       "LYNNWOOD - 212TH                                                  2079\n",
       "Tukwila Allentown                                                 2074\n",
       "BELLEVUE -  BELLEVUE WAY NE                                       1443\n",
       "Mount Rainier National Park, Tahoma Woods                         1105\n",
       "Tulalip-Totem Beach Rd                                            1068\n",
       "Tulalip-Tulalip Tribe                                              701\n",
       "PUYALLUP-66TH AVE E (PUYALLUP TRIBE)                               638\n",
       "SEATTLE - OLIVE ST                                                 549\n",
       "Auburn M St SE                                                     439\n",
       "ENUMCLAW - MUD MTN (Army Corp of Engineers site)                   241\n",
       "ISSAQUAH -  LAKE SAMMAMISH (Wiithin Lake Sammamish State Park)     128\n",
       "Name: city_name, dtype: int64"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seattle.city_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "2f948124-955e-464a-aa5f-2d49fe173418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 57227 entries, 0 to 67807\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   city_name  56502 non-null  object \n",
      " 1   date       57227 non-null  object \n",
      " 2   pm2_5      57227 non-null  float64\n",
      " 3   latitude   57227 non-null  float64\n",
      " 4   longitude  57227 non-null  float64\n",
      "dtypes: float64(3), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_seattle.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "0ecfb02d-fbb4-4eef-9849-372293cf5b20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bellevue-SE 12th St', 'DARRINGTON - FIR ST (Darrington High School)', 'KENT - JAMES & CENTRAL', 'LAKE FOREST PARK TOWNE CENTER', 'MARYSVILLE - 7TH AVE (Marysville Junior High)', 'NORTH BEND - NORTH BEND WAY', 'SEATTLE - BEACON HILL', 'SEATTLE - DUWAMISH', 'SEATTLE - SOUTH PARK #2', 'Seattle-10th & Weller', 'TACOMA - ALEXANDER AVE', 'TACOMA - L STREET', 'Tacoma-S 36th St', 'Tukwila Allentown', 'Tulalip-Totem Beach Rd']\n"
     ]
    }
   ],
   "source": [
    "df_seattle['date'] = pd.to_datetime(df_seattle['date'])\n",
    "\n",
    "# Group the dataframe by city_name and count the number of rows for each group\n",
    "counts = df_seattle.groupby('city_name').size()\n",
    "\n",
    "# Filter the stations that have more than 1000 rows\n",
    "filtered_counts = counts[counts > 1000]\n",
    "\n",
    "# Get the city_names of the filtered stations\n",
    "filtered_sites = filtered_counts.index\n",
    "\n",
    "# Filter the dataframe to only include rows from the filtered stations\n",
    "filtered_df = df_seattle[df_seattle['city_name'].isin(filtered_sites)]\n",
    "\n",
    "# Filter the dataframe to only include rows with a date newer than 2022-08-01\n",
    "filtered_df = filtered_df[filtered_df['date'] > '2022-08-01']\n",
    "\n",
    "# Get the city_names of the filtered stations that have at least one row newer than 2022-08-01\n",
    "newer_sites = filtered_df['city_name'].unique().tolist()\n",
    "\n",
    "# Create a list of stations that meet both criteria\n",
    "sites_to_leave = [site for site in filtered_sites if site in newer_sites]\n",
    "\n",
    "# Print the station list\n",
    "print(sites_to_leave)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd6c74-9e15-4ae9-87d6-73b9a498cb12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Considering data quantity and the freshness of data for each site, I decided to cut off some sites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "b9916cea-b577-4d99-b306-6d369b4a1c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_seattle = df_seattle[df_seattle.city_name.isin(sites_to_leave)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "2be49bd9-d2da-4de0-b79c-1f66f145d777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_seattle = df_seattle.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "256eaca8-3f86-485e-8ba2-b3fc682bc554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46479, 5)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seattle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "a12c0914-a68b-4503-8341-fa068b25728e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lets rename these sites so we could later concat this df with other cities data\n",
    "\n",
    "df_seattle.city_name= df_seattle.city_name.apply(lambda x: \"Seattle - \" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "d9f13125-4bc5-41d2-baa9-e2d9f2e758f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seattle - NORTH BEND - NORTH BEND WAY                      3705\n",
       "Seattle - TACOMA - L STREET                                3696\n",
       "Seattle - SEATTLE - BEACON HILL                            3691\n",
       "Seattle - MARYSVILLE - 7TH AVE (Marysville Junior High)    3648\n",
       "Seattle - DARRINGTON - FIR ST (Darrington High School)     3614\n",
       "Seattle - SEATTLE - SOUTH PARK #2                          3577\n",
       "Seattle - TACOMA - ALEXANDER AVE                           3569\n",
       "Seattle - KENT - JAMES & CENTRAL                           3556\n",
       "Seattle - SEATTLE - DUWAMISH                               3439\n",
       "Seattle - Seattle-10th & Weller                            3097\n",
       "Seattle - LAKE FOREST PARK TOWNE CENTER                    2999\n",
       "Seattle - Tacoma-S 36th St                                 2574\n",
       "Seattle - Bellevue-SE 12th St                              2172\n",
       "Seattle - Tukwila Allentown                                2074\n",
       "Seattle - Tulalip-Totem Beach Rd                           1068\n",
       "Name: city_name, dtype: int64"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seattle.city_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "ff6f5b08-64fa-4c0e-820f-7cbc0d6e5d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_seattle['latitude'] = df_seattle['latitude'].apply(lambda x: round(x, 5))\n",
    "# df_seattle['longitude'] = df_seattle['longitude'].apply(lambda x: round(x, 5))\n",
    "\n",
    "# df_seattle[\"coordinates\"] = tuple(zip(df_seattle[\"latitude\"], df_seattle[\"longitude\"]))\n",
    "df_seattle = df_seattle.drop(columns=[\"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "5c7abb60-8d68-4ed6-93cf-56cbf4f15fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>date</th>\n",
       "      <th>pm2_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39055</th>\n",
       "      <td>Seattle - DARRINGTON - FIR ST (Darrington High...</td>\n",
       "      <td>2021-12-12</td>\n",
       "      <td>13.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               city_name       date  pm2_5\n",
       "39055  Seattle - DARRINGTON - FIR ST (Darrington High... 2021-12-12   13.8"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seattle.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "9169654f-0b9a-4290-9205-5097ea111702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_seattle.to_csv(\"data/backfill_pm2_5_seattle.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "2c06115c-c308-4b82-b3b4-5e26f7e06683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_seattle = pd.read_csv(\"data/backfill_pm2_5_seattle.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "4912a3c0-d1c1-445a-a505-dd627601eae6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>date</th>\n",
       "      <th>pm2_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seattle - NORTH BEND - NORTH BEND WAY</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seattle - NORTH BEND - NORTH BEND WAY</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle - NORTH BEND - NORTH BEND WAY</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               city_name        date  pm2_5\n",
       "0  Seattle - NORTH BEND - NORTH BEND WAY  2013-01-01    4.7\n",
       "1  Seattle - NORTH BEND - NORTH BEND WAY  2013-01-02    2.8\n",
       "2  Seattle - NORTH BEND - NORTH BEND WAY  2013-01-03    3.2"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seattle.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "0cca61b2-bc5c-4044-9849-18d95f723a98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46479, 3)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seattle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e142e2b-9981-471b-bf44-ebc475039950",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Weather data from [open meteo](https://open-meteo.com/en/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36c812-55a1-42bf-a8b4-2f4bd9d838b2",
   "metadata": {},
   "source": [
    "- Maximum Temperature (2 m)\n",
    "- Minimum Temperature (2 m)\n",
    "- Precipitation Sum\n",
    "- Rain Sum\n",
    "- Snowfall Sum\n",
    "- Precipitation Hours\n",
    "- Maximum Wind Speed (10 m)\n",
    "- Maximum Wind Gusts (10 m)\n",
    "- Dominant Wind Direction (10 m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "a6882fe7-9a31-4571-ac03-abfede7a2a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_weather_data_from_open_meteo(city_name: str,\n",
    "                                     start_date: str,\n",
    "                                     end_date: str,\n",
    "                                     coordinates: list = None,\n",
    "                                     forecast: bool = False):\n",
    "    \"\"\"\n",
    "    Takes [city name OR coordinates] and returns pandas DataFrame with weather data.\n",
    "    \n",
    "    Examples of arguments:\n",
    "        coordinates=(47.755, -122.2806), start_date=\"2023-01-01\"\n",
    "    \"\"\"\n",
    "    start_of_cell = time.time()\n",
    "    \n",
    "    if coordinates:\n",
    "        latitude, longitude = coordinates\n",
    "    else:\n",
    "        latitude, longitude = get_city_coordinates(city_name=city_name)\n",
    "    \n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'daily': [\"temperature_2m_max\", \"temperature_2m_min\",\n",
    "                  \"precipitation_sum\", \"rain_sum\", \"snowfall_sum\",\n",
    "                  \"precipitation_hours\", \"windspeed_10m_max\",\n",
    "                  \"windgusts_10m_max\", \"winddirection_10m_dominant\"],\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'timezone': \"Europe/London\"\n",
    "    }\n",
    "    \n",
    "    if forecast:\n",
    "        # historical forecast endpoint\n",
    "        base_url = 'https://api.open-meteo.com/v1/forecast' \n",
    "    else:\n",
    "        # historical observations endpoint\n",
    "        base_url = 'https://archive-api.open-meteo.com/v1/archive' \n",
    "        \n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    response_json = response.json()    \n",
    "    res_df = pd.DataFrame(response_json[\"daily\"])\n",
    "    \n",
    "    res_df[\"city_name\"] = city_name\n",
    "    \n",
    "    # rename columns\n",
    "    res_df = res_df.rename(columns={\n",
    "        \"time\": \"date\",\n",
    "        \"temperature_2m_max\": \"temperature_max\",\n",
    "        \"temperature_2m_min\": \"temperature_min\",\n",
    "        \"windspeed_10m_max\": \"wind_speed_max\",\n",
    "        \"winddirection_10m_dominant\": \"wind_direction_dominant\",\n",
    "        \"windgusts_10m_max\": \"wind_gusts_max\"\n",
    "    })\n",
    "    \n",
    "    # change columns order\n",
    "    res_df = res_df[\n",
    "        ['city_name', 'date', 'temperature_max', 'temperature_min',\n",
    "         'precipitation_sum', 'rain_sum', 'snowfall_sum',\n",
    "         'precipitation_hours', 'wind_speed_max',\n",
    "         'wind_gusts_max', 'wind_direction_dominant']\n",
    "    ]\n",
    "    \n",
    "    # convert dates in 'date' column\n",
    "    res_df[\"date\"] = pd.to_datetime(res_df[\"date\"])\n",
    "    \n",
    "#     # create 'unix' columns\n",
    "#     res_df[\"unix_time\"] = res_df[\"base_time\"].apply(convert_date_to_unix)\n",
    "    end_of_cell = time.time()\n",
    "    print(f\"Parsed weather for {city_name} since {start_date} till {end_date}.\")\n",
    "    print(f\"Took {round(end_of_cell - start_of_cell, 2)} sec.\\n\")\n",
    "        \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "06223174-e52e-46ae-af2b-fdd8a31c1507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed weather for some site in Seattle since 2023-01-01 till 2023-01-01.\n",
      "Took 0.35 sec.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_max</th>\n",
       "      <th>temperature_min</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>rain_sum</th>\n",
       "      <th>snowfall_sum</th>\n",
       "      <th>precipitation_hours</th>\n",
       "      <th>wind_speed_max</th>\n",
       "      <th>wind_gusts_max</th>\n",
       "      <th>wind_direction_dominant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>some site in Seattle</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>8.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>14.8</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              city_name       date  temperature_max  temperature_min  \\\n",
       "0  some site in Seattle 2023-01-01              8.1              3.9   \n",
       "\n",
       "   precipitation_sum  rain_sum  snowfall_sum  precipitation_hours  \\\n",
       "0                2.2       2.2           0.0                 10.0   \n",
       "\n",
       "   wind_speed_max  wind_gusts_max  wind_direction_dominant  \n",
       "0             7.8            14.8                      140  "
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weather_data_from_open_meteo(city_name=\"some site in Seattle\", \n",
    "                                 coordinates=(47.755, -122.2806),\n",
    "                                 start_date=\"2023-01-01\", end_date=\"2023-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "f46882dc-05e5-441b-8d05-0b8f5c22b616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed weather for Phoenix-Mesa since 2023-01-01 till 2023-01-01.\n",
      "Took 0.54 sec.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_max</th>\n",
       "      <th>temperature_min</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>rain_sum</th>\n",
       "      <th>snowfall_sum</th>\n",
       "      <th>precipitation_hours</th>\n",
       "      <th>wind_speed_max</th>\n",
       "      <th>wind_gusts_max</th>\n",
       "      <th>wind_direction_dominant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phoenix-Mesa</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>17.3</td>\n",
       "      <td>11.6</td>\n",
       "      <td>21.6</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.4</td>\n",
       "      <td>39.6</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city_name       date  temperature_max  temperature_min  \\\n",
       "0  Phoenix-Mesa 2023-01-01             17.3             11.6   \n",
       "\n",
       "   precipitation_sum  rain_sum  snowfall_sum  precipitation_hours  \\\n",
       "0               21.6      32.4           0.0                  7.0   \n",
       "\n",
       "   wind_speed_max  wind_gusts_max  wind_direction_dominant  \n",
       "0            26.4            39.6                      129  "
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weather_data_from_open_meteo(city_name=\"Phoenix-Mesa\", start_date=\"2023-01-01\", end_date=\"2023-01-01\", forecast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "3f954e00-42c0-48dc-a98b-16ec589119d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "today = datetime.date.today() # datetime object\n",
    "\n",
    "day7next = str(today + datetime.timedelta(7))\n",
    "day7ago = str(today - datetime.timedelta(7))\n",
    "tomorrow = str(today + datetime.timedelta(1))\n",
    "\n",
    "start_date = \"2013-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "30f21de8-c97b-4a58-9146-c4fe11a47cea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weather data for ALL cities (and Seattle sites)\n",
    "df_weather = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "c087ba48-73e8-4410-9d71-70f5ba81fdf9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed weather for Amsterdam since 2013-01-01 till 2023-04-06.\n",
      "Took 1.99 sec.\n",
      "\n",
      "Parsed weather for Athina since 2013-01-01 till 2023-04-06.\n",
      "Took 2.1 sec.\n",
      "\n",
      "Parsed weather for Berlin since 2013-01-01 till 2023-04-06.\n",
      "Took 0.84 sec.\n",
      "\n",
      "Parsed weather for Gdansk since 2013-01-01 till 2023-04-06.\n",
      "Took 1.3 sec.\n",
      "\n",
      "Parsed weather for Kraków since 2013-01-01 till 2023-04-06.\n",
      "Took 0.86 sec.\n",
      "\n",
      "Parsed weather for London since 2013-01-01 till 2023-04-06.\n",
      "Took 0.82 sec.\n",
      "\n",
      "Parsed weather for Madrid since 2013-01-01 till 2023-04-06.\n",
      "Took 0.82 sec.\n",
      "\n",
      "Parsed weather for Marseille since 2013-01-01 till 2023-04-06.\n",
      "Took 0.87 sec.\n",
      "\n",
      "Parsed weather for Milano since 2013-01-01 till 2023-04-06.\n",
      "Took 1.1 sec.\n",
      "\n",
      "Parsed weather for München since 2013-01-01 till 2023-04-06.\n",
      "Took 0.86 sec.\n",
      "\n",
      "Parsed weather for Napoli since 2013-01-01 till 2023-04-06.\n",
      "Took 0.82 sec.\n",
      "\n",
      "Parsed weather for Paris since 2013-01-01 till 2023-04-06.\n",
      "Took 0.85 sec.\n",
      "\n",
      "Parsed weather for Sevilla since 2013-01-01 till 2023-04-06.\n",
      "Took 0.81 sec.\n",
      "\n",
      "Parsed weather for Stockholm since 2013-01-01 till 2023-04-06.\n",
      "Took 0.82 sec.\n",
      "\n",
      "Parsed weather for Tallinn since 2013-01-01 till 2023-04-06.\n",
      "Took 0.91 sec.\n",
      "\n",
      "Parsed weather for Varna since 2013-01-01 till 2023-04-06.\n",
      "Took 0.88 sec.\n",
      "\n",
      "Parsed weather for Wien since 2013-01-01 till 2023-04-06.\n",
      "Took 0.91 sec.\n",
      "\n",
      "Parsed weather for Albuquerque since 2013-01-01 till 2023-04-06.\n",
      "Took 3.64 sec.\n",
      "\n",
      "Parsed weather for Atlanta since 2013-01-01 till 2023-04-06.\n",
      "Took 1.11 sec.\n",
      "\n",
      "Parsed weather for Chicago since 2013-01-01 till 2023-04-06.\n",
      "Took 0.88 sec.\n",
      "\n",
      "Parsed weather for Columbus since 2013-01-01 till 2023-04-06.\n",
      "Took 0.8 sec.\n",
      "\n",
      "Parsed weather for Dallas since 2013-01-01 till 2023-04-06.\n",
      "Took 0.82 sec.\n",
      "\n",
      "Parsed weather for Denver since 2013-01-01 till 2023-04-06.\n",
      "Took 0.85 sec.\n",
      "\n",
      "Parsed weather for Houston since 2013-01-01 till 2023-04-06.\n",
      "Took 0.81 sec.\n",
      "\n",
      "Parsed weather for Los Angeles since 2013-01-01 till 2023-04-06.\n",
      "Took 0.85 sec.\n",
      "\n",
      "Parsed weather for New York since 2013-01-01 till 2023-04-06.\n",
      "Took 0.81 sec.\n",
      "\n",
      "Parsed weather for Phoenix-Mesa since 2013-01-01 till 2023-04-06.\n",
      "Took 0.82 sec.\n",
      "\n",
      "Parsed weather for Salt Lake City since 2013-01-01 till 2023-04-06.\n",
      "Took 0.79 sec.\n",
      "\n",
      "Parsed weather for San Francisco since 2013-01-01 till 2023-04-06.\n",
      "Took 0.88 sec.\n",
      "\n",
      "Parsed weather for Tampa since 2013-01-01 till 2023-04-06.\n",
      "Took 0.84 sec.\n",
      "\n",
      "Parsed weather for Amsterdam since 2023-04-06 till 2023-04-13.\n",
      "Took 0.51 sec.\n",
      "\n",
      "Parsed weather for Athina since 2023-04-06 till 2023-04-13.\n",
      "Took 0.51 sec.\n",
      "\n",
      "Parsed weather for Berlin since 2023-04-06 till 2023-04-13.\n",
      "Took 0.52 sec.\n",
      "\n",
      "Parsed weather for Gdansk since 2023-04-06 till 2023-04-13.\n",
      "Took 0.48 sec.\n",
      "\n",
      "Parsed weather for Kraków since 2023-04-06 till 2023-04-13.\n",
      "Took 0.53 sec.\n",
      "\n",
      "Parsed weather for London since 2023-04-06 till 2023-04-13.\n",
      "Took 0.49 sec.\n",
      "\n",
      "Parsed weather for Madrid since 2023-04-06 till 2023-04-13.\n",
      "Took 0.51 sec.\n",
      "\n",
      "Parsed weather for Marseille since 2023-04-06 till 2023-04-13.\n",
      "Took 0.51 sec.\n",
      "\n",
      "Parsed weather for Milano since 2023-04-06 till 2023-04-13.\n",
      "Took 0.61 sec.\n",
      "\n",
      "Parsed weather for München since 2023-04-06 till 2023-04-13.\n",
      "Took 0.47 sec.\n",
      "\n",
      "Parsed weather for Napoli since 2023-04-06 till 2023-04-13.\n",
      "Took 0.49 sec.\n",
      "\n",
      "Parsed weather for Paris since 2023-04-06 till 2023-04-13.\n",
      "Took 0.55 sec.\n",
      "\n",
      "Parsed weather for Sevilla since 2023-04-06 till 2023-04-13.\n",
      "Took 0.48 sec.\n",
      "\n",
      "Parsed weather for Stockholm since 2023-04-06 till 2023-04-13.\n",
      "Took 0.49 sec.\n",
      "\n",
      "Parsed weather for Tallinn since 2023-04-06 till 2023-04-13.\n",
      "Took 0.53 sec.\n",
      "\n",
      "Parsed weather for Varna since 2023-04-06 till 2023-04-13.\n",
      "Took 0.52 sec.\n",
      "\n",
      "Parsed weather for Wien since 2023-04-06 till 2023-04-13.\n",
      "Took 0.55 sec.\n",
      "\n",
      "Parsed weather for Albuquerque since 2023-04-06 till 2023-04-13.\n",
      "Took 0.52 sec.\n",
      "\n",
      "Parsed weather for Atlanta since 2023-04-06 till 2023-04-13.\n",
      "Took 0.57 sec.\n",
      "\n",
      "Parsed weather for Chicago since 2023-04-06 till 2023-04-13.\n",
      "Took 0.56 sec.\n",
      "\n",
      "Parsed weather for Columbus since 2023-04-06 till 2023-04-13.\n",
      "Took 0.52 sec.\n",
      "\n",
      "Parsed weather for Dallas since 2023-04-06 till 2023-04-13.\n",
      "Took 0.52 sec.\n",
      "\n",
      "Parsed weather for Denver since 2023-04-06 till 2023-04-13.\n",
      "Took 0.45 sec.\n",
      "\n",
      "Parsed weather for Houston since 2023-04-06 till 2023-04-13.\n",
      "Took 0.5 sec.\n",
      "\n",
      "Parsed weather for Los Angeles since 2023-04-06 till 2023-04-13.\n",
      "Took 0.54 sec.\n",
      "\n",
      "Parsed weather for New York since 2023-04-06 till 2023-04-13.\n",
      "Took 0.46 sec.\n",
      "\n",
      "Parsed weather for Phoenix-Mesa since 2023-04-06 till 2023-04-13.\n",
      "Took 0.49 sec.\n",
      "\n",
      "Parsed weather for Salt Lake City since 2023-04-06 till 2023-04-13.\n",
      "Took 0.49 sec.\n",
      "\n",
      "Parsed weather for San Francisco since 2023-04-06 till 2023-04-13.\n",
      "Took 0.57 sec.\n",
      "\n",
      "Parsed weather for Tampa since 2023-04-06 till 2023-04-13.\n",
      "Took 0.46 sec.\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Parsed historical weather data for EU and US cities up to 2023-04-13.\n",
      "Took 47.47 sec.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_of_cell = time.time()\n",
    "\n",
    "for city_name in target_cities[\"EU\"] + target_cities[\"US\"]:\n",
    "    df_ = get_weather_data_from_open_meteo(city_name=city_name,\n",
    "                                           start_date=start_date,\n",
    "                                           end_date=day7ago,\n",
    "                                           forecast=False) # firstly I am parsing historical weather data\n",
    "    df_weather = pd.concat([df_weather, df_]).reset_index(drop=True)\n",
    "\n",
    "for city_name in target_cities[\"EU\"] + target_cities[\"US\"]:\n",
    "    df_ = get_weather_data_from_open_meteo(city_name=city_name,\n",
    "                                           start_date=day7ago,\n",
    "                                           end_date=str(today),\n",
    "                                           forecast=True) # now forecasts till today\n",
    "    df_weather = pd.concat([df_weather, df_]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "end_of_cell = time.time()\n",
    "print(\"-\" * 64)\n",
    "print(f\"Parsed historical weather data for EU and US cities up to {str(today)}.\")\n",
    "print(f\"Took {round(end_of_cell - start_of_cell, 2)} sec.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "5731137e-399c-4734-b0f9-571c4945e2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed weather for Seattle - NORTH BEND - NORTH BEND WAY since 2013-01-01 till 2023-04-06.\n",
      "Took 0.62 sec.\n",
      "\n",
      "Parsed weather for Seattle - LAKE FOREST PARK TOWNE CENTER since 2013-01-01 till 2023-04-06.\n",
      "Took 0.52 sec.\n",
      "\n",
      "Parsed weather for Seattle - SEATTLE - DUWAMISH since 2013-01-01 till 2023-04-06.\n",
      "Took 0.59 sec.\n",
      "\n",
      "Parsed weather for Seattle - SEATTLE - BEACON HILL since 2013-01-01 till 2023-04-06.\n",
      "Took 0.58 sec.\n",
      "\n",
      "Parsed weather for Seattle - SEATTLE - SOUTH PARK #2 since 2013-01-01 till 2023-04-06.\n",
      "Took 0.61 sec.\n",
      "\n",
      "Parsed weather for Seattle - KENT - JAMES & CENTRAL since 2013-01-01 till 2023-04-06.\n",
      "Took 0.62 sec.\n",
      "\n",
      "Parsed weather for Seattle - TACOMA - L STREET since 2013-01-01 till 2023-04-06.\n",
      "Took 0.61 sec.\n",
      "\n",
      "Parsed weather for Seattle - TACOMA - ALEXANDER AVE since 2013-01-01 till 2023-04-06.\n",
      "Took 0.6 sec.\n",
      "\n",
      "Parsed weather for Seattle - DARRINGTON - FIR ST (Darrington High School) since 2013-01-01 till 2023-04-06.\n",
      "Took 0.63 sec.\n",
      "\n",
      "Parsed weather for Seattle - MARYSVILLE - 7TH AVE (Marysville Junior High) since 2013-01-01 till 2023-04-06.\n",
      "Took 0.6 sec.\n",
      "\n",
      "Parsed weather for Seattle - Seattle-10th & Weller since 2013-01-01 till 2023-04-06.\n",
      "Took 0.61 sec.\n",
      "\n",
      "Parsed weather for Seattle - Bellevue-SE 12th St since 2013-01-01 till 2023-04-06.\n",
      "Took 0.65 sec.\n",
      "\n",
      "Parsed weather for Seattle - Tacoma-S 36th St since 2013-01-01 till 2023-04-06.\n",
      "Took 0.56 sec.\n",
      "\n",
      "Parsed weather for Seattle - Tukwila Allentown since 2013-01-01 till 2023-04-06.\n",
      "Took 0.57 sec.\n",
      "\n",
      "Parsed weather for Seattle - Tulalip-Totem Beach Rd since 2013-01-01 till 2023-04-06.\n",
      "Took 0.62 sec.\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Parsed weather data for Seattle and surrounding areas up to 2023-04-13.\n",
      "Took 9.58 sec.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_of_cell = time.time()\n",
    "\n",
    "df_seattle_update = pd.DataFrame()\n",
    "for city_name in target_cities[\"Seattle\"]:\n",
    "    coordinates = target_cities[\"Seattle\"][city_name]\n",
    "    df_ = get_weather_data_from_open_meteo(city_name=city_name,\n",
    "                                           coordinates=coordinates,\n",
    "                                           start_date=start_date,\n",
    "                                           end_date=day7ago,\n",
    "                                           forecast=False) # firstly I am parsing historical weather data\n",
    "    df_weather = pd.concat([df_weather, df_]).reset_index(drop=True)\n",
    "    \n",
    "for city_name in target_cities[\"Seattle\"]:\n",
    "    coordinates = target_cities[\"Seattle\"][city_name]\n",
    "    df_ = get_weather_data_from_open_meteo(city_name=city_name,\n",
    "                                           coordinates=coordinates,\n",
    "                                           start_date=day7ago,\n",
    "                                           end_date=str(today),\n",
    "                                           forecast=True) # now forecasts till today\n",
    "    df_weather = pd.concat([df_weather, df_]).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "end_of_cell = time.time()\n",
    "print(\"-\" * 64)\n",
    "print(f\"Parsed weather data for Seattle and surrounding areas up to {str(today)}.\")\n",
    "print(f\"Took {round(end_of_cell - start_of_cell, 2)} sec.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "a58f0871-2afd-4e88-b0c9-37c6ac64e8ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_weather.date = pd.to_datetime(df_weather.date)\n",
    "df_weather.date = df_weather.date.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "72eec57f-9d6b-4aad-86dd-7502d9b6a6cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_weather = df_weather.drop_duplicates([\"city_name\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "d8301fd4-009b-469e-9755-ade45cc0c353",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Albuquerque</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amsterdam</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Athina</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atlanta</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Berlin</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chicago</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Columbus</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dallas</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Denver</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gdansk</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Houston</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kraków</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>London</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Los Angeles</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madrid</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marseille</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Milano</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>München</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Napoli</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paris</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix-Mesa</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Salt Lake City</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Francisco</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - Bellevue-SE 12th St</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - DARRINGTON - FIR ST (Darrington High School)</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - KENT - JAMES &amp; CENTRAL</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - LAKE FOREST PARK TOWNE CENTER</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - MARYSVILLE - 7TH AVE (Marysville Junior High)</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - NORTH BEND - NORTH BEND WAY</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - SEATTLE - BEACON HILL</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - SEATTLE - DUWAMISH</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - SEATTLE - SOUTH PARK #2</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - Seattle-10th &amp; Weller</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - TACOMA - ALEXANDER AVE</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - TACOMA - L STREET</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - Tacoma-S 36th St</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - Tukwila Allentown</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle - Tulalip-Totem Beach Rd</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sevilla</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stockholm</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tallinn</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tampa</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Varna</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wien</th>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    date\n",
       "city_name                                               \n",
       "Albuquerque                                         3755\n",
       "Amsterdam                                           3755\n",
       "Athina                                              3755\n",
       "Atlanta                                             3755\n",
       "Berlin                                              3755\n",
       "Chicago                                             3755\n",
       "Columbus                                            3755\n",
       "Dallas                                              3755\n",
       "Denver                                              3755\n",
       "Gdansk                                              3755\n",
       "Houston                                             3755\n",
       "Kraków                                              3755\n",
       "London                                              3755\n",
       "Los Angeles                                         3755\n",
       "Madrid                                              3755\n",
       "Marseille                                           3755\n",
       "Milano                                              3755\n",
       "München                                             3755\n",
       "Napoli                                              3755\n",
       "New York                                            3755\n",
       "Paris                                               3755\n",
       "Phoenix-Mesa                                        3755\n",
       "Salt Lake City                                      3755\n",
       "San Francisco                                       3755\n",
       "Seattle - Bellevue-SE 12th St                       3755\n",
       "Seattle - DARRINGTON - FIR ST (Darrington High ...  3755\n",
       "Seattle - KENT - JAMES & CENTRAL                    3755\n",
       "Seattle - LAKE FOREST PARK TOWNE CENTER             3755\n",
       "Seattle - MARYSVILLE - 7TH AVE (Marysville Juni...  3755\n",
       "Seattle - NORTH BEND - NORTH BEND WAY               3755\n",
       "Seattle - SEATTLE - BEACON HILL                     3755\n",
       "Seattle - SEATTLE - DUWAMISH                        3755\n",
       "Seattle - SEATTLE - SOUTH PARK #2                   3755\n",
       "Seattle - Seattle-10th & Weller                     3755\n",
       "Seattle - TACOMA - ALEXANDER AVE                    3755\n",
       "Seattle - TACOMA - L STREET                         3755\n",
       "Seattle - Tacoma-S 36th St                          3755\n",
       "Seattle - Tukwila Allentown                         3755\n",
       "Seattle - Tulalip-Totem Beach Rd                    3755\n",
       "Sevilla                                             3755\n",
       "Stockholm                                           3755\n",
       "Tallinn                                             3755\n",
       "Tampa                                               3755\n",
       "Varna                                               3755\n",
       "Wien                                                3755"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather[['city_name', 'date']].groupby('city_name').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "4f7648b7-f7f4-4d70-834b-620a1cc37338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_weather.to_csv(\"data/backfill_weather.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d4aeb-450f-4be1-b463-2e921bcb11ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af0568-d9ac-4d0d-a890-bfee8988a5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6529c-18a7-4d6b-b600-81c74ccdbc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2a8a011-621b-4753-a69d-c788afa888db",
   "metadata": {},
   "source": [
    "# TODO: Data engineering - MOVE IT TO THE NEXT NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1c15b-5747-4baf-8feb-b5eb4ef2d4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0977229-c49e-46c3-a211-57a8bccaafcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d8b3d-3be7-47be-8655-5fd3d244bce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a91359-c007-4dfe-bde5-0d25e7bb8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data engineering\n",
    "\n",
    "def moving_average(df, window=7):\n",
    "    df[f'mean_{window}_days'] = df.groupby('station_id')['users_count'] \\\n",
    "                                    .rolling(window=window).mean().reset_index(0,drop=True).shift(1)\n",
    "    return df\n",
    "\n",
    "# def moving_average(df, window=7):\n",
    "#     df[f'mean_{window}_days'] = df[\"users_count\"].rolling(window=window).mean()\n",
    "#     return df\n",
    "\n",
    "\n",
    "def moving_std(df, window):\n",
    "    df[f'std_{window}_days'] = df.groupby('station_id')['users_count'] \\\n",
    "                                    .rolling(window=window).std().reset_index(0,drop=True).shift(1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def exponential_moving_average(df, window):\n",
    "    df[f'exp_mean_{window}_days'] = df.groupby('station_id')['users_count'].ewm(span=window) \\\n",
    "                                        .mean().reset_index(0,drop=True).shift(1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def exponential_moving_std(df, window):\n",
    "    df[f'exp_std_{window}_days'] = df.groupby('station_id')['users_count'].ewm(span=window) \\\n",
    "                                        .std().reset_index(0,drop=True).shift(1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def engineer_citibike_features(df):\n",
    "    df_res = df.copy()\n",
    "    # there are duplicated rows (several records for the same day and station). get rid of it.\n",
    "    df_res = df_res.groupby(['date', 'station_id'], as_index=False)['users_count'].sum()\n",
    "\n",
    "    df_res['prev_users_count'] = df_res.groupby('station_id')['users_count'].shift(+1)\n",
    "    df_res = df_res.dropna()\n",
    "    df_res = moving_average(df_res, 7)\n",
    "    df_res = moving_average(df_res, 14)\n",
    "\n",
    "\n",
    "    for i in [7, 14]:\n",
    "        for func in [moving_std, exponential_moving_average,\n",
    "                     exponential_moving_std\n",
    "                     ]:\n",
    "            df_res = func(df_res, i)\n",
    "    df_res = df_res.reset_index(drop=True)\n",
    "    return df_res.sort_values(by=[\"date\", \"station_id\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5d54b-f709-475b-9413-e8b6d5c9dbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b913f4e-b093-4201-95ae-986a915a9d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
